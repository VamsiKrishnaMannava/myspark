{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4Hohy6FU0qSJ",
        "U282tTdj0j5e",
        "jN630Ui-3NRG",
        "NttANhGtBq1y",
        "HJbRZT4hLw32",
        "dz85-7HPNOSs",
        "YKwLK3EsO9Id"
      ],
      "authorship_tag": "ABX9TyM6ymgxOnD9/v7MhSpe0lwZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VamsiKrishnaMannava/myspark/blob/main/Pyspark_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "bZVz_D1NlWpG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66410e83-7ad4-4eb8-db0a-2fe80fab048c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=febd56cd2f257fd4d694234b2304844b0172bd037df5924659250b9528dfe2b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(name=\"Test-1\").getOrCreate()"
      ],
      "metadata": {
        "id": "UkrfJ7wvDHFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pys"
      ],
      "metadata": {
        "id": "vsFiweAkr_qS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario-1 multi delimiters"
      ],
      "metadata": {
        "id": "4Hohy6FU0qSJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1bsJPj-lNNp"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import split, col"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"1,Alice\\t30|New York\",\"2,Jonny\\t36|New Addington\",]"
      ],
      "metadata": {
        "id": "DBoxJrCElTC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data, \"string\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "yb-SfuHUmGW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_col = split(df['value'],',|\\t|\\|')"
      ],
      "metadata": {
        "id": "ei0HzngImTf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('id',split_col.getItem(0)).withColumn('name',split_col.getItem(1)).withColumn('age',split_col.getItem(2)).withColumn('city', split_col.getItem(3))"
      ],
      "metadata": {
        "id": "Oebqi1oSnaMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "Gw3Lih_loKGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "ZDm13URkpSYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('id', 'name', 'age', 'city').show()"
      ],
      "metadata": {
        "id": "sdes1yVPo4nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario-2 find missing"
      ],
      "metadata": {
        "id": "U282tTdj0j5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = [(1,),(2,),(4,),(5,),(7,),(8,),(10,)]\n",
        "\n",
        "df_numbers = spark.createDataFrame(data2,[\"Number\"])"
      ],
      "metadata": {
        "id": "fIULTAU70nYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_numbers.show()"
      ],
      "metadata": {
        "id": "RzoAMzhM1UmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_range = spark.range(1,11).toDF(\"Number\")"
      ],
      "metadata": {
        "id": "pW4hRWCb1ajY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_range.show()"
      ],
      "metadata": {
        "id": "QbdlAcBV1ujw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missingNumber = full_range.join(df_numbers,\"Number\",\"left_anti\")"
      ],
      "metadata": {
        "id": "rYl1H9Do1wzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missingNumber.sort([\"Number\"]).show()"
      ],
      "metadata": {
        "id": "GB9Q-vht1_wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario-3 Top3 movies based on rating"
      ],
      "metadata": {
        "id": "jN630Ui-3NRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_movies = [(1, \"Movie A\"),(2, \"Movie B\"),(3, \"Movie C\"),(4, \"Movie D\"),(5, \"Movie E\")]\n",
        "columns_movies = [\"MoviesID\", \"MovieName\"]"
      ],
      "metadata": {
        "id": "XSpDGuBb2CJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_movies = spark.createDataFrame(data_movies, columns_movies)"
      ],
      "metadata": {
        "id": "V4g2owD13u2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_movies.sort(\"MoviesID\").show()\n",
        "df_movies.sort(\"MoviesID\").take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Rk19qbo37Xh",
        "outputId": "78f90eb0-69a2-4cee-d2d7-c07276ef6b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+\n",
            "|MoviesID|MovieName|\n",
            "+--------+---------+\n",
            "|       1|  Movie A|\n",
            "|       2|  Movie B|\n",
            "|       3|  Movie C|\n",
            "|       4|  Movie D|\n",
            "|       5|  Movie E|\n",
            "+--------+---------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(MoviesID=1, MovieName='Movie A'),\n",
              " Row(MoviesID=2, MovieName='Movie B'),\n",
              " Row(MoviesID=3, MovieName='Movie C')]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_ratings = [ (1, 101, 4.5), (1, 102, 4.0), (2, 103, 5.0),\n",
        "                 (2, 104, 3.5), (3, 105, 4.0), (3, 106, 4.0),\n",
        "                 (4, 107, 3.0), (5, 108, 2.5), (5, 109, 3.0)]\n",
        "\n",
        "columns_ratings = [\"MoviesID\",\"UserID\",\"Rating\"]\n",
        "\n",
        "df_ratings = spark.createDataFrame(data_ratings, columns_ratings)\n",
        "\n",
        "df_ratings.sort(\"MoviesID\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R0_uVK93_JK",
        "outputId": "b85d49bc-340a-46be-86b2-87a6297f0156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+\n",
            "|MoviesID|UserID|Rating|\n",
            "+--------+------+------+\n",
            "|       1|   101|   4.5|\n",
            "|       1|   102|   4.0|\n",
            "|       2|   103|   5.0|\n",
            "|       2|   104|   3.5|\n",
            "|       3|   105|   4.0|\n",
            "|       3|   106|   4.0|\n",
            "|       4|   107|   3.0|\n",
            "|       5|   108|   2.5|\n",
            "|       5|   109|   3.0|\n",
            "+--------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n"
      ],
      "metadata": {
        "id": "Fpe307ja5847"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_movies.createOrReplaceTempView(\"tab_movies\")\n",
        "df_ratings.createOrReplaceTempView(\"tab_ratings\")"
      ],
      "metadata": {
        "id": "J4wOwNoR6buC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"select * from tab_movies\"\n",
        "query2 = \"select * from tab_ratings\"\n",
        "\n",
        "spark.sql(query1).show()\n",
        "spark.sql(query2).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teZxtlDO63WT",
        "outputId": "107658a4-2dc8-4ce0-ae5f-dd438967a362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+\n",
            "|MoviesID|MovieName|\n",
            "+--------+---------+\n",
            "|       1|  Movie A|\n",
            "|       2|  Movie B|\n",
            "|       3|  Movie C|\n",
            "|       4|  Movie D|\n",
            "|       5|  Movie E|\n",
            "+--------+---------+\n",
            "\n",
            "+--------+------+------+\n",
            "|MoviesID|UserID|Rating|\n",
            "+--------+------+------+\n",
            "|       1|   101|   4.5|\n",
            "|       1|   102|   4.0|\n",
            "|       2|   103|   5.0|\n",
            "|       2|   104|   3.5|\n",
            "|       3|   105|   4.0|\n",
            "|       3|   106|   4.0|\n",
            "|       4|   107|   3.0|\n",
            "|       5|   108|   2.5|\n",
            "|       5|   109|   3.0|\n",
            "+--------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query3 = \"\"\"\n",
        "select a.MoviesID, a.MovieName, avg(b.Rating) as Avg_Rating from tab_movies a, tab_ratings b where a.MoviesID = b.MoviesID group by a.MoviesID, a.MovieName\n",
        "\"\"\"\n",
        "spark.sql(query3).show()\n",
        "spark.sql(query3).createOrReplaceTempView(\"total_ratings\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agTOERqf8ZU1",
        "outputId": "e73b9c87-afc5-40b8-853f-062a672b390e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+----------+\n",
            "|MoviesID|MovieName|Avg_Rating|\n",
            "+--------+---------+----------+\n",
            "|       1|  Movie A|      4.25|\n",
            "|       2|  Movie B|      4.25|\n",
            "|       3|  Movie C|       4.0|\n",
            "|       4|  Movie D|       3.0|\n",
            "|       5|  Movie E|      2.75|\n",
            "+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query4 = \"\"\"\n",
        "select * from total_ratings order by Avg_Rating desc limit 3\n",
        "\"\"\"\n",
        "spark.sql(query4).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-KktLxa-C9v",
        "outputId": "97222b76-192e-4890-ac2d-b7ddf02fd229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+----------+\n",
            "|MoviesID|MovieName|Avg_Rating|\n",
            "+--------+---------+----------+\n",
            "|       1|  Movie A|      4.25|\n",
            "|       2|  Movie B|      4.25|\n",
            "|       3|  Movie C|       4.0|\n",
            "+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario-4 Rolling Avg past 7 days"
      ],
      "metadata": {
        "id": "NttANhGtBq1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as func"
      ],
      "metadata": {
        "id": "TA957fMU_DBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salesdata = [\n",
        "    Row(Date='2023-01-01', ProductID=100, QuantitySold=10),\n",
        "    Row(Date='2023-01-02', ProductID=100, QuantitySold=15),\n",
        "    Row(Date='2023-01-03', ProductID=100, QuantitySold=20),\n",
        "    Row(Date='2023-01-04', ProductID=100, QuantitySold=25),\n",
        "    Row(Date='2023-01-05', ProductID=100, QuantitySold=30),\n",
        "    Row(Date='2023-01-06', ProductID=100, QuantitySold=35),\n",
        "    Row(Date='2023-01-07', ProductID=100, QuantitySold=40),\n",
        "    Row(Date='2023-01-08', ProductID=100, QuantitySold=45)\n",
        "]\n",
        "\n",
        "df_sales = spark.createDataFrame(salesdata)\n",
        "\n",
        "df_sales.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O4Od_IHC-ML",
        "outputId": "b2195347-27fb-4c95-dd7d-ad6cfa2a6983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+------------+\n",
            "|      Date|ProductID|QuantitySold|\n",
            "+----------+---------+------------+\n",
            "|2023-01-01|      100|          10|\n",
            "|2023-01-02|      100|          15|\n",
            "|2023-01-03|      100|          20|\n",
            "|2023-01-04|      100|          25|\n",
            "|2023-01-05|      100|          30|\n",
            "|2023-01-06|      100|          35|\n",
            "|2023-01-07|      100|          40|\n",
            "|2023-01-08|      100|          45|\n",
            "+----------+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales = df_sales.withColumn(\"Date\", func.to_date(func.col(\"Date\")))\n",
        "\n",
        "df_sales.show()\n",
        "\n",
        "df_sales.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no0q47yuEIT8",
        "outputId": "27cf790f-fd12-41ab-883b-1bc7761562a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+------------+\n",
            "|      Date|ProductID|QuantitySold|\n",
            "+----------+---------+------------+\n",
            "|2023-01-01|      100|          10|\n",
            "|2023-01-02|      100|          15|\n",
            "|2023-01-03|      100|          20|\n",
            "|2023-01-04|      100|          25|\n",
            "|2023-01-05|      100|          30|\n",
            "|2023-01-06|      100|          35|\n",
            "|2023-01-07|      100|          40|\n",
            "|2023-01-08|      100|          45|\n",
            "+----------+---------+------------+\n",
            "\n",
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- ProductID: long (nullable = true)\n",
            " |-- QuantitySold: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windowsspec = Window.partitionBy('ProductID').orderBy('Date').rowsBetween(-6,0)\n",
        "\n",
        "rollingavg = df_sales.withColumn('7DayAvg', func.avg('QuantitySold').over(windowsspec))\n",
        "rollingavg.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiO6w1HJEgEE",
        "outputId": "5ca45504-539d-4f5a-ca7a-7d35ce6f5a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+------------+-------+\n",
            "|      Date|ProductID|QuantitySold|7DayAvg|\n",
            "+----------+---------+------------+-------+\n",
            "|2023-01-01|      100|          10|   10.0|\n",
            "|2023-01-02|      100|          15|   12.5|\n",
            "|2023-01-03|      100|          20|   15.0|\n",
            "|2023-01-04|      100|          25|   17.5|\n",
            "|2023-01-05|      100|          30|   20.0|\n",
            "|2023-01-06|      100|          35|   22.5|\n",
            "|2023-01-07|      100|          40|   25.0|\n",
            "|2023-01-08|      100|          45|   30.0|\n",
            "+----------+---------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# samething in sql\n",
        "\n",
        "df_sales.createOrReplaceTempView(\"rollavg\")\n",
        "query5 = \"\"\"\n",
        "select\n",
        "  a.*,\n",
        "  avg(QuantitySold)\n",
        "    over(partition by ProductID order by Date\n",
        "        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as 7dayavg\n",
        "  from rollAVG a\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query5).show()\n",
        "\n",
        "spark.sql(query5).printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZrkXkFpH0M3",
        "outputId": "83b8f05d-621b-4475-f103-52f868ecd0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+------------+-------+\n",
            "|      Date|ProductID|QuantitySold|7dayavg|\n",
            "+----------+---------+------------+-------+\n",
            "|2023-01-01|      100|          10|   10.0|\n",
            "|2023-01-02|      100|          15|   12.5|\n",
            "|2023-01-03|      100|          20|   15.0|\n",
            "|2023-01-04|      100|          25|   17.5|\n",
            "|2023-01-05|      100|          30|   20.0|\n",
            "|2023-01-06|      100|          35|   22.5|\n",
            "|2023-01-07|      100|          40|   25.0|\n",
            "|2023-01-08|      100|          45|   30.0|\n",
            "+----------+---------+------------+-------+\n",
            "\n",
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- ProductID: long (nullable = true)\n",
            " |-- QuantitySold: long (nullable = true)\n",
            " |-- 7dayavg: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario-5 : Catagorising data using UDFs"
      ],
      "metadata": {
        "id": "HJbRZT4hLw32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType"
      ],
      "metadata": {
        "id": "7qKi6Qc3IVOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data3 = [\n",
        "    Row(UserID=4001, Age=17),\n",
        "    Row(UserID=4002, Age=45),\n",
        "    Row(UserID=4003, Age=65),\n",
        "    Row(UserID=4004, Age=30),\n",
        "    Row(UserID=4005, Age=80)\n",
        "]\n",
        "\n",
        "df_cat = spark.createDataFrame(data3)\n",
        "\n",
        "df_cat.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLBpDGeWMVKS",
        "outputId": "24abf31b-b5dc-4097-84a0-33a5366a9a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+\n",
            "|UserID|Age|\n",
            "+------+---+\n",
            "|  4001| 17|\n",
            "|  4002| 45|\n",
            "|  4003| 65|\n",
            "|  4004| 30|\n",
            "|  4005| 80|\n",
            "+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def catagorize_age(age):\n",
        "  if age < 18:\n",
        "    return 'Youth'\n",
        "  elif age < 60:\n",
        "    return 'Adult'\n",
        "  else:\n",
        "    return 'Senior'\n",
        "\n",
        "age_udf = udf(catagorize_age, StringType())"
      ],
      "metadata": {
        "id": "KcMzLw6UNNQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cat = df_cat.withColumn(\"AgeGroup\", age_udf(df_cat[\"Age\"]))\n",
        "df_cat.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQMkqeItytQo",
        "outputId": "0e90ea46-04e4-41a6-858b-a7abf539279f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+--------+\n",
            "|UserID|Age|AgeGroup|\n",
            "+------+---+--------+\n",
            "|4001  |17 |Youth   |\n",
            "|4002  |45 |Adult   |\n",
            "|4003  |65 |Senior  |\n",
            "|4004  |30 |Adult   |\n",
            "|4005  |80 |Senior  |\n",
            "+------+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iCtxaU0Nyu-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenaroio-6 read json"
      ],
      "metadata": {
        "id": "dz85-7HPNOSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_json = spark.read.json(\"/content/sample_data/anscombe.json\")\n",
        "df_json.show()\n",
        "\n",
        "df_json.createOrReplaceTempView(\"jsondata\")\n",
        "\n",
        "query6=\"select Series, X,Y from jsondata where x is not null\"\n",
        "spark.sql(query6).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9vSZa9pM7wa",
        "outputId": "92156580-1426-4e6f-b0bd-7d14032fb778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  NULL|NULL| NULL|              [|\n",
            "|     I|10.0| 8.04|           NULL|\n",
            "|     I| 8.0| 6.95|           NULL|\n",
            "|     I|13.0| 7.58|           NULL|\n",
            "|     I| 9.0| 8.81|           NULL|\n",
            "|     I|11.0| 8.33|           NULL|\n",
            "|     I|14.0| 9.96|           NULL|\n",
            "|     I| 6.0| 7.24|           NULL|\n",
            "|     I| 4.0| 4.26|           NULL|\n",
            "|     I|12.0|10.84|           NULL|\n",
            "|     I| 7.0| 4.81|           NULL|\n",
            "|     I| 5.0| 5.68|           NULL|\n",
            "|    II|10.0| 9.14|           NULL|\n",
            "|    II| 8.0| 8.14|           NULL|\n",
            "|    II|13.0| 8.74|           NULL|\n",
            "|    II| 9.0| 8.77|           NULL|\n",
            "|    II|11.0| 9.26|           NULL|\n",
            "|    II|14.0|  8.1|           NULL|\n",
            "|    II| 6.0| 6.13|           NULL|\n",
            "|    II| 4.0|  3.1|           NULL|\n",
            "+------+----+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+------+----+-----+\n",
            "|Series|   X|    Y|\n",
            "+------+----+-----+\n",
            "|     I|10.0| 8.04|\n",
            "|     I| 8.0| 6.95|\n",
            "|     I|13.0| 7.58|\n",
            "|     I| 9.0| 8.81|\n",
            "|     I|11.0| 8.33|\n",
            "|     I|14.0| 9.96|\n",
            "|     I| 6.0| 7.24|\n",
            "|     I| 4.0| 4.26|\n",
            "|     I|12.0|10.84|\n",
            "|     I| 7.0| 4.81|\n",
            "|     I| 5.0| 5.68|\n",
            "|    II|10.0| 9.14|\n",
            "|    II| 8.0| 8.14|\n",
            "|    II|13.0| 8.74|\n",
            "|    II| 9.0| 8.77|\n",
            "|    II|11.0| 9.26|\n",
            "|    II|14.0|  8.1|\n",
            "|    II| 6.0| 6.13|\n",
            "|    II| 4.0|  3.1|\n",
            "|    II|12.0| 9.13|\n",
            "+------+----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenaroio-7 read csv"
      ],
      "metadata": {
        "id": "YKwLK3EsO9Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv = spark.read.csv(\"/content/sample_data/california_housing_test.csv\", inferSchema=True, header=True)\n",
        "df_csv.show()\n",
        "df_csv.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUS-fTDgNe8p",
        "outputId": "4ed34b64-5995-4f7f-8868-46712c259fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|  -122.05|   37.37|              27.0|     3885.0|         661.0|    1537.0|     606.0|       6.6085|          344700.0|\n",
            "|   -118.3|   34.26|              43.0|     1510.0|         310.0|     809.0|     277.0|        3.599|          176500.0|\n",
            "|  -117.81|   33.78|              27.0|     3589.0|         507.0|    1484.0|     495.0|       5.7934|          270500.0|\n",
            "|  -118.36|   33.82|              28.0|       67.0|          15.0|      49.0|      11.0|       6.1359|          330000.0|\n",
            "|  -119.67|   36.33|              19.0|     1241.0|         244.0|     850.0|     237.0|       2.9375|           81700.0|\n",
            "|  -119.56|   36.51|              37.0|     1018.0|         213.0|     663.0|     204.0|       1.6635|           67000.0|\n",
            "|  -121.43|   38.63|              43.0|     1009.0|         225.0|     604.0|     218.0|       1.6641|           67000.0|\n",
            "|  -120.65|   35.48|              19.0|     2310.0|         471.0|    1341.0|     441.0|        3.225|          166900.0|\n",
            "|  -122.84|    38.4|              15.0|     3080.0|         617.0|    1446.0|     599.0|       3.6696|          194400.0|\n",
            "|  -118.02|   34.08|              31.0|     2402.0|         632.0|    2830.0|     603.0|       2.3333|          164200.0|\n",
            "|  -118.24|   33.98|              45.0|      972.0|         249.0|    1288.0|     261.0|       2.2054|          125000.0|\n",
            "|  -119.12|   35.85|              37.0|      736.0|         166.0|     564.0|     138.0|       2.4167|           58300.0|\n",
            "|  -121.93|   37.25|              36.0|     1089.0|         182.0|     535.0|     170.0|         4.69|          252600.0|\n",
            "|  -117.03|   32.97|              16.0|     3936.0|         694.0|    1935.0|     659.0|       4.5625|          231200.0|\n",
            "|  -117.97|   33.73|              27.0|     2097.0|         325.0|    1217.0|     331.0|       5.7121|          222500.0|\n",
            "|  -117.99|   33.81|              42.0|      161.0|          40.0|     157.0|      50.0|          2.2|          153100.0|\n",
            "|  -120.81|   37.53|              15.0|      570.0|         123.0|     189.0|     107.0|        1.875|          181300.0|\n",
            "|   -121.2|   38.69|              26.0|     3077.0|         607.0|    1603.0|     595.0|       2.7174|          137500.0|\n",
            "|  -118.88|   34.21|              26.0|     1590.0|         196.0|     654.0|     199.0|       6.5851|          300000.0|\n",
            "|  -122.59|   38.01|              35.0|     8814.0|        1307.0|    3450.0|    1258.0|       6.1724|          414300.0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- median_house_value: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario-8 : first purchase date of each user\n"
      ],
      "metadata": {
        "id": "XZteFCajHPvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.functions import min"
      ],
      "metadata": {
        "id": "j9gm_cz3HVxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "purchase_data = [\n",
        "    Row(UserID=1, purchase_date='2023-01-05'),\n",
        "    Row(UserID=1, purchase_date='2023-01-10'),\n",
        "    Row(UserID=2, purchase_date='2023-01-03'),\n",
        "    Row(UserID=3, purchase_date='2023-01-12')\n",
        "]\n",
        "\n",
        "df_purchase = spark.createDataFrame(purchase_data)"
      ],
      "metadata": {
        "id": "rqzHgqRpNGWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_purchase.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFw7vxYyNhhi",
        "outputId": "16ce89c9-2bac-4ae5-8187-f5e9659cbcbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+\n",
            "|UserID|purchase_date|\n",
            "+------+-------------+\n",
            "|     1|   2023-01-05|\n",
            "|     1|   2023-01-10|\n",
            "|     2|   2023-01-03|\n",
            "|     3|   2023-01-12|\n",
            "+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_purchase2 = df_purchase.withColumn(\"purchase_date\", f.to_date(f.col(\"purchase_date\")))\n",
        "df_purchase2.createOrReplaceTempView(\"purchases\")"
      ],
      "metadata": {
        "id": "6llw2mJ2Njoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using sql\n",
        "query = \"select UserID, min(purchase_date) from purchases group by UserId order by UserID\"\n",
        "spark.sql(query).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6reyZiRPHMz",
        "outputId": "5006122c-f325-4ea9-e580-b7813298d08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------+\n",
            "|UserID|min(purchase_date)|\n",
            "+------+------------------+\n",
            "|     1|        2023-01-05|\n",
            "|     2|        2023-01-03|\n",
            "|     3|        2023-01-12|\n",
            "+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_firstpurchase = df_purchase2.groupBy(\"UserID\").agg(min(\"purchase_date\").alias(\"FirstPurchase_date\")).orderBy(\"UserID\")\n",
        "\n",
        "df_firstpurchase.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-OeyyPRRdrl",
        "outputId": "4ec4d350-b910-4ef8-f16b-44bac75b874b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------+\n",
            "|UserID|FirstPurchase_date|\n",
            "+------+------------------+\n",
            "|     1|        2023-01-05|\n",
            "|     2|        2023-01-03|\n",
            "|     3|        2023-01-12|\n",
            "+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario-9: replace nulls with mean"
      ],
      "metadata": {
        "id": "gWP4aj9Gt7Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean, col"
      ],
      "metadata": {
        "id": "g_VMW16awNM-"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data = [(\"1\",100),(\"2\",150),(\"3\",None),(\"4\",200),(\"5\",None)]\n",
        "\n",
        "df_sales = spark.createDataFrame(sales_data, [\"sales_id\",\"amount\"])\n",
        "df_sales.show()"
      ],
      "metadata": {
        "id": "6oI29OMdRzXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25eb494-a26f-4537-be48-e45988f27636"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+\n",
            "|sales_id|amount|\n",
            "+--------+------+\n",
            "|       1|   100|\n",
            "|       2|   150|\n",
            "|       3|  NULL|\n",
            "|       4|   200|\n",
            "|       5|  NULL|\n",
            "+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sql way\n",
        "df_sales.createOrReplaceTempView(\"meanreplace\")\n",
        "#query = \"select sales_id, nvl(amount, mean(nvl(amount,0)) over (order by 1)) as meanamt from meanreplace\"\n",
        "\n",
        "query = \"\"\"\n",
        "with mntx as\n",
        "  (select mean(amount) mnt from meanreplace where amount is not null)\n",
        "\n",
        "select a.sales_id, nvl(a.amount, x.mnt) as meanamt from meanreplace a, mntx x\n",
        "\"\"\"\n",
        "spark.sql(query).show()  #answer"
      ],
      "metadata": {
        "id": "Db_aBv6ISODF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a5819c-e0af-4ea7-fda6-04154612e000"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+\n",
            "|sales_id|meanamt|\n",
            "+--------+-------+\n",
            "|       1|  100.0|\n",
            "|       2|  150.0|\n",
            "|       3|  150.0|\n",
            "|       4|  200.0|\n",
            "|       5|  150.0|\n",
            "+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_value = df_sales.na.drop().agg(mean(col(\"amount\")))\n",
        "avg_value = avg_value.first()[0]\n",
        "\n",
        "df_fullsales = df_sales.na.fill(avg_value)\n",
        "\n",
        "df_fullsales.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqquruJNwFWZ",
        "outputId": "bbb1ed9f-5466-4c26-8e75-9db6d9957d52"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+\n",
            "|sales_id|amount|\n",
            "+--------+------+\n",
            "|       1|   100|\n",
            "|       2|   150|\n",
            "|       3|   150|\n",
            "|       4|   200|\n",
            "|       5|   150|\n",
            "+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario-10: Split data by month\n"
      ],
      "metadata": {
        "id": "pMUilp755GgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "data = [\n",
        "    Row(\"Product1\", 100,150,200),\n",
        "    Row(\"Product2\", 200,250,300),\n",
        "    Row(\"Product3\", 300,350,400)\n",
        "]\n",
        "\n",
        "df_qtrdata = spark.createDataFrame(data, [\"product\",\"month1\",\"month2\",\"month3\"])\n",
        "\n",
        "df_qtrdata.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB4YxC46ww5L",
        "outputId": "d62ee8e9-9ed3-4c9e-abe9-402e1ed2e0a4"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+------+\n",
            "| product|month1|month2|month3|\n",
            "+--------+------+------+------+\n",
            "|Product1|   100|   150|   200|\n",
            "|Product2|   200|   250|   300|\n",
            "|Product3|   300|   350|   400|\n",
            "+--------+------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_qtrdata.selectExpr(\"product\",\"stack(3, 'jan', month1, 'feb', month2, 'mar', month3)as (Month, sales)\" ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_bf7bF66hoV",
        "outputId": "41a985cb-ea27-48a8-ffad-70e2176d11f7"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+-----+\n",
            "| product|Month|sales|\n",
            "+--------+-----+-----+\n",
            "|Product1|  jan|  100|\n",
            "|Product1|  feb|  150|\n",
            "|Product1|  mar|  200|\n",
            "|Product2|  jan|  200|\n",
            "|Product2|  feb|  250|\n",
            "|Product2|  mar|  300|\n",
            "|Product3|  jan|  300|\n",
            "|Product3|  feb|  350|\n",
            "|Product3|  mar|  400|\n",
            "+--------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YHsWYJUG79A8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}